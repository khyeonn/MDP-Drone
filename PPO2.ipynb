{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Drone2.\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: ArgumentError: Package LinearAlgebra not found in current path, maybe you meant `import/using ..LinearAlgebra`.\n- Otherwise, run `import Pkg; Pkg.add(\"LinearAlgebra\")` to install the LinearAlgebra package.\nin expression starting at /Users/andres/Documents/UMD/Spring 2024/ENAE 788Z/Project/MDP-Drone/Drone2.jl:1",
     "output_type": "error",
     "traceback": [
      "LoadError: ArgumentError: Package LinearAlgebra not found in current path, maybe you meant `import/using ..LinearAlgebra`.\n",
      "- Otherwise, run `import Pkg; Pkg.add(\"LinearAlgebra\")` to install the LinearAlgebra package.\n",
      "in expression starting at /Users/andres/Documents/UMD/Spring 2024/ENAE 788Z/Project/MDP-Drone/Drone2.jl:1\n",
      "\n",
      "Stacktrace:\n",
      " [1] macro expansion\n",
      "   @ ./loading.jl:1772 [inlined]\n",
      " [2] macro expansion\n",
      "   @ ./lock.jl:267 [inlined]\n",
      " [3] __require(into::Module, mod::Symbol)\n",
      "   @ Base ./loading.jl:1753\n",
      " [4] #invoke_in_world#3\n",
      "   @ ./essentials.jl:926 [inlined]\n",
      " [5] invoke_in_world\n",
      "   @ ./essentials.jl:923 [inlined]\n",
      " [6] require(into::Module, mod::Symbol)\n",
      "   @ Base ./loading.jl:1746\n",
      " [7] include(fname::String)\n",
      "   @ Base.MainInclude ./client.jl:489\n",
      " [8] top-level scope\n",
      "   @ ~/Documents/UMD/Spring 2024/ENAE 788Z/Project/MDP-Drone/PPO2.ipynb:1"
     ]
    }
   ],
   "source": [
    "include(\"Drone2.jl\")\n",
    "using .Drone2: DroneMDP, DroneAct, DroneState, render, gen, isterminal, reset!, act!\n",
    "using StaticArrays\n",
    "using LinearAlgebra\n",
    "using Statistics\n",
    "using Random\n",
    "using Distributions: Normal, logpdf\n",
    "using Flux\n",
    "using Flux: params, gradient, update!, gradient, Optimise, Adam, mse, train\n",
    "using ElectronDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed Forward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCritic"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "struct ActorCritic\n",
    "    actor::Chain\n",
    "    critic::Chain\n",
    "end\n",
    "\n",
    "function ActorCritic(state_size::Int, action_size::Int)\n",
    "    actor = Chain(\n",
    "        Dense(state_size, 64, sigmoid),\n",
    "        Dense(64, action_size * 2),\n",
    "        x -> [x[1],x[2], softplus.(x[3]),softplus.(x[4])]\n",
    "    )\n",
    "\n",
    "    critic = Chain(\n",
    "        Dense(state_size, 64, sigmoid),\n",
    "        Dense(64, 1)\n",
    "    )\n",
    "\n",
    "    return ActorCritic(actor, critic)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function forward(ac::ActorCritic, states)\n",
    "    if isa(states, DroneState)\n",
    "        states = [states]\n",
    "    end\n",
    "    results = Vector{Tuple{Vector{Float64}, Vector{Float64}}}(undef, length(states))\n",
    "    values = zeros(length(states), 1)\n",
    "    for i in 1:length(states)\n",
    "        actor_out = ac.actor(states[i])  \n",
    "        value = ac.critic(states[i])[1]\n",
    "        action_mean = [actor_out[1]; actor_out[2]]\n",
    "        action_std = [actor_out[3]; actor_out[4]]\n",
    "        results[i] = (action_mean, action_std)\n",
    "        values[i] = value\n",
    "    end\n",
    "    return results, values\n",
    "end \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "Base.Meta.ParseError",
     "evalue": "ParseError:\n# Error @ /Users/andres/Documents/UMD/Spring 2024/ENAE 788Z/Project/MDP-Drone/PPO2.ipynb:23:4\n\nend\n#  └ ── Expected `end`",
     "output_type": "error",
     "traceback": [
      "ParseError:\n",
      "# Error @ /Users/andres/Documents/UMD/Spring 2024/ENAE 788Z/Project/MDP-Drone/PPO2.ipynb:23:4\n",
      "\n",
      "end\n",
      "#  └ ── Expected `end`\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/UMD/Spring 2024/ENAE 788Z/Project/MDP-Drone/PPO2.ipynb:23"
     ]
    }
   ],
   "source": [
    "function learn(ac::ActorCritic,m::DroneMDP, total_timesteps::Int64,time_steps_per_batch::Int64, max_timesteps_per_episode::Int64, n_updates_per_iteration::Int64,clip=0.2 )\n",
    "    t_so_far = 0\n",
    "    while t_so_far < total_timesteps\n",
    "        batch_states, batch_actions, batch_log_probs, batch_rtgs, batch_lens = rollout(ac, m, time_steps_per_batch, max_timesteps_per_episode)\n",
    "        \n",
    "        V,_ = evaluate(ac, batch_states, batch_actions)\n",
    "\n",
    "        A_k = advantage(batch_rtgs, V)\n",
    "        for _ in 1:n_updates_per_iteration\n",
    "            # Epoch code\n",
    "            actor_loss_fn(ac,batch_states,batch_actions,batch_log_probs,A_k,clip)\n",
    "\n",
    "\n",
    "            actor_opt = Adam(lr)\n",
    "            actor_gs = gradient(() -> actor_loss )\n",
    "            update!(Adam(0.005), params(ac.actor), actor_gs)\n",
    "\n",
    "            critic_opt = Adam(lr)\n",
    "            critic_gs = gradient(() -> critic_loss_fn(ppo_network, batch_obs, batch_rtgo), params(ppo_network.critic.model))\n",
    "            update!(critic_opt, params(ppo_network.critic.model), critic_gs)\n",
    "\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actor_loss_fn (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function actor_loss_fn(batch_states,batch_actions,batch_log_probs)\n",
    "    _,curr_log_probs = evaluate(ac, batch_states, batch_actions)\n",
    "    ratios = exp.(curr_log_probs-batch_log_probs)\n",
    "    surr1 = ratios.*A_k\n",
    "    surr2 = clamp.(surr1,1-clip,1+clip)\n",
    "    actor_loss = mean(-min(surr1,surr2))\n",
    "    return actor_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rollout(ac::ActorCritic, m::DroneMDP, time_steps_per_batch::Int64, max_timesteps_per_episode::Int64)\n",
    "    batch_states = []\n",
    "    batch_actions = []\n",
    "    batch_log_probs =[]\n",
    "    batch_rews = []\n",
    "    batch_lens = []\n",
    "\n",
    "    t = 0\n",
    "    while t < time_steps_per_batch\n",
    "        ep_rews =[]\n",
    "        s = reset!(m)\n",
    "        done = false\n",
    "        ep_t_temp = 0\n",
    "        for ep_t in 1:max_timesteps_per_episode\n",
    "            t += 1\n",
    "            push!(batch_states,s)\n",
    "            action, log_prob = get_action(ac::ActorCritic, m::DroneMDP)\n",
    "            s, rew, done = act!(m,action)\n",
    "            render(m)\n",
    "            push!(ep_rews,rew)\n",
    "            push!(batch_actions,action)\n",
    "            push!(batch_log_probs,log_prob)\n",
    "            ep_t_temp =ep_t\n",
    "            if done\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        push!(batch_lens,ep_t_temp+1)\n",
    "        push!(batch_rews,ep_rews)\n",
    "    end\n",
    "\n",
    "    batch_rtgs = rewards2go(m,batch_rews)\n",
    "    \n",
    "    return   batch_states, batch_actions, batch_log_probs, batch_rtgs, batch_lens\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_action (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function get_action(ac::ActorCritic, m::DroneMDP)\n",
    "    result = forward(ac, m.drone)\n",
    "    action_mean = result[1][1][1] \n",
    "    action_std = result[1][1][2]\n",
    "    params = rand.(Normal.(action_mean, action_std))\n",
    "    v = clamp(params[1], -m.v_max, m.v_max)\n",
    "    omega = clamp(params[2], -m.om_max, m.om_max)\n",
    "    action = DroneAct(v,omega)\n",
    "    log_prob = sum(logpdf.(Normal.(action_mean, action_std), action))\n",
    "    return action, log_prob\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rewards2go (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function rewards2go(m::DroneMDP, batch_rews)\n",
    "    batch_rtg = []\n",
    "    for ep_rews in reverse(batch_rews)\n",
    "        discounted_reward = 0\n",
    "        for rew in  reverse(ep_rews)\n",
    "            discounted_reward = rew + m.discount*discounted_reward\n",
    "            pushfirst!(batch_rtg,discounted_reward)\n",
    "        end\n",
    "    end\n",
    "    return  batch_rtg\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_prob (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function log_prob(ac::ActorCritic,batch_states,batch_actions)\n",
    "    log_probs = zeros(length(batch_states))\n",
    "    for i in 1:length(batch_states)\n",
    "        result = forward(ac, batch_states[i])\n",
    "        action_mean = result[1][1][1] \n",
    "        action_std = result[1][1][2]\n",
    "        log_prob = sum(logpdf.(Normal.(action_mean, action_std), batch_actions[i]))\n",
    "        log_probs[i] =  log_prob\n",
    "    end\n",
    "    return log_probs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function evaluate(ac::ActorCritic, batch_states, batch_actions)\n",
    "   V = zeros(length(batch_states))\n",
    "   for i in 1:length(batch_states)\n",
    "      V[i] = ac.critic(batch_states[i])[1]\n",
    "   end\n",
    "   log_probs = log_prob(ac,batch_states,batch_actions)\n",
    "   return V, log_probs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "advantage (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function advantage(batch_rtgs, V)\n",
    "    A = batch_rtgs-V\n",
    "    return normalize(A)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ac = ActorCritic(3,2)\n",
    "m = DroneMDP()\n",
    "clip = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Any[Float32[25.0, 25.0, 0.0], Float32[24.408594, 25.211927, 5.9390965], Float32[23.861311, 25.408241, 5.938776], Float32[23.37253, 25.958403, 5.438776], Float32[23.256216, 26.463379, 4.938776], Float32[23.15758, 27.111712, 4.86337], Float32[22.786339, 27.599062, 5.36337], Float32[22.281893, 27.824223, 5.86337], Float32[21.637169, 28.178728, 5.7804527], Float32[21.066261, 28.60882, 5.637546]  …  Float32[9.938222, 10.040588, 0.9311141], Float32[9.414979, 9.799912, 0.43111408], Float32[8.970353, 9.202337, 0.9311141], Float32[8.337469, 8.91123, 0.43111408], Float32[7.7758946, 8.949976, 6.214299], Float32[7.219337, 9.305838, 5.714299], Float32[6.76973, 9.815548, 5.435217], Float32[6.5367723, 10.662055, 4.9809403], Float32[6.2947435, 11.316176, 5.066774], Float32[5.6538653, 11.874214, 5.566774]], Any[Float32[-1.2564622, -0.6881776], Float32[-1.1628556, -0.00064091414], Float32[-1.471847, -1.0], Float32[-1.0363995, -1.0], Float32[-1.3115875, -0.15081252], Float32[-1.225285, 1.0], Float32[-1.1048309, 1.0], Float32[-1.4715197, -0.16583472], Float32[-1.4295651, -0.28581333], Float32[-1.431804, -1.0]  …  Float32[-1.1518829, -1.0], Float32[-1.4896817, 1.0], Float32[-1.3932488, -1.0], Float32[-1.1258187, -1.0], Float32[-1.3212032, -1.0], Float32[-1.3593394, -0.5581647], Float32[-1.7559541, -0.90855354], Float32[-1.3949239, 0.1716673], Float32[-1.6995649, 1.0], Float32[-1.601162, -1.0]], Any[-0.7940089860343693, -0.7193076355305765, -2.0043228122155736, -1.4913661692387563, -0.7921082710483407, -0.7224364583081896, -0.9115869518472202, -2.0455717259768904, -1.661206040153666, -1.836277202569856  …  -1.7491444181199016, -1.217122643619261, -0.8961165040691701, -1.9803955087388905, -1.3809928692141242, -1.1182596992793965, -1.4938852447450344, -0.9291774201269882, -1.73634522553448, -1.0362595381287554], Any[-67.57308f0, -70.07693f0, -72.712555f0, -75.4869f0, -78.407265f0, -81.48133f0, -84.71719f0, -88.12336f0, -91.7088f0, -95.48295f0  …  54.999676f0, 58.94703f0, 63.102135f0, 67.47593f0, 72.079926f0, 76.92624f0, 82.02762f0, 87.39749f0, 93.049995f0, 99.0f0], Any[61, 34, 59])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_states, batch_actions, batch_log_probs, batch_rtgs, batch_lens = rollout(π, m, 100, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010881271502685403"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "V,_ = evaluate(π, batch_states, batch_actions)\n",
    "A_k = advantage(batch_rtgs, V)\n",
    "actor_loss_fn(batch_states,batch_actions,batch_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010881271502685403"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data =  (batch_states,batch_actions,batch_log_probs)\n",
    "actor_loss_fn(batch_states,batch_actions,batch_log_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
