{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# first import the POMDPs.jl interface\n",
    "using POMDPs\n",
    "using Plots\n",
    "# POMDPModelTools has tools that help build the MDP definition\n",
    "using POMDPModelTools\n",
    "# POMDPPolicies provides functions to help define simple policies\n",
    "using POMDPPolicies\n",
    "# POMDPSimulators provide functions for running MDP simulations\n",
    "using POMDPSimulators\n",
    "using ElectronDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct DroneState\n",
    "    x::Int64 # x position\n",
    "    y::Int64 # y position\n",
    "    theta::Float64 # heading angle\n",
    "    done::Bool # are we in a terminal state?\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "posequal (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initial state constructor\n",
    "DroneState(x::Int64, y::Int64, θ::Float64) = DroneState(x,y,θ,false)\n",
    "# checks if the position of two states are the same\n",
    "posequal(s1::DroneState, s2::DroneState) = s1.x == s2.x && s1.y == s2.y && s1.theta == s2.theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the grid world mdp type\n",
    "mutable struct DroneEnv <: MDP{DroneState, Symbol}\n",
    "    size_x::Int64 # x size of the grid\n",
    "    size_y::Int64 # y size of the grid\n",
    "    size_theta::Int64 # theta size\n",
    "    reward_states::Vector{DroneState} # the states in which agent recieves reward\n",
    "    reward_values::Vector{Float64} # reward values for those states\n",
    "    tprob::Float64 # probability of transitioning to the desired state\n",
    "    discount_factor::Float64 # disocunt factor\n",
    "    target::Vector{Int64}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108-element Vector{DroneState}:\n",
       " DroneState(10, 10, 0.0, false)\n",
       " DroneState(10, 10, 1.5707963267948966, false)\n",
       " DroneState(10, 10, 3.141592653589793, false)\n",
       " DroneState(10, 10, 4.71238898038469, false)\n",
       " DroneState(5, 10, 0.0, false)\n",
       " DroneState(5, 10, 1.5707963267948966, false)\n",
       " DroneState(5, 10, 3.141592653589793, false)\n",
       " DroneState(5, 10, 4.71238898038469, false)\n",
       " DroneState(5, 9, 0.0, false)\n",
       " DroneState(5, 9, 1.5707963267948966, false)\n",
       " ⋮\n",
       " DroneState(5, 2, 4.71238898038469, false)\n",
       " DroneState(6, 2, 0.0, false)\n",
       " DroneState(6, 2, 1.5707963267948966, false)\n",
       " DroneState(6, 2, 3.141592653589793, false)\n",
       " DroneState(6, 2, 4.71238898038469, false)\n",
       " DroneState(7, 2, 0.0, false)\n",
       " DroneState(7, 2, 1.5707963267948966, false)\n",
       " DroneState(7, 2, 3.141592653589793, false)\n",
       " DroneState(7, 2, 4.71238898038469, false)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#we use key worded arguments so we can change any of the values we pass in \n",
    "function DroneEnv(;sx::Int64=10, # size_x\n",
    "                   sy::Int64=10, # size_y\n",
    "                   stheta::Int64=4, # size_theta\n",
    "                   rs::Vector{DroneState}=[DroneState(10, 10, 0.0), DroneState(10, 10, 1.5707963267948966), DroneState(10, 10, 3.141592653589793), DroneState(10, 10, 4.71238898038469), DroneState(5, 10, 0.0), DroneState(5, 10, 1.5707963267948966), DroneState(5, 10, 3.141592653589793), DroneState(5, 10, 4.71238898038469), DroneState(5, 9, 0.0), DroneState(5, 9, 1.5707963267948966), DroneState(5, 9, 3.141592653589793), DroneState(5, 9, 4.71238898038469), DroneState(10, 9, 0.0), DroneState(10, 9, 1.5707963267948966), DroneState(10, 9, 3.141592653589793), DroneState(10, 9, 4.71238898038469), DroneState(2, 8, 0.0), DroneState(2, 8, 1.5707963267948966), DroneState(2, 8, 3.141592653589793), DroneState(2, 8, 4.71238898038469), DroneState(3, 8, 0.0), DroneState(3, 8, 1.5707963267948966), DroneState(3, 8, 3.141592653589793), DroneState(3, 8, 4.71238898038469), DroneState(10, 8, 0.0), DroneState(10, 8, 1.5707963267948966), DroneState(10, 8, 3.141592653589793), DroneState(10, 8, 4.71238898038469), DroneState(2, 7, 0.0), DroneState(2, 7, 1.5707963267948966), DroneState(2, 7, 3.141592653589793), DroneState(2, 7, 4.71238898038469), DroneState(3, 7, 0.0), DroneState(3, 7, 1.5707963267948966), DroneState(3, 7, 3.141592653589793), DroneState(3, 7, 4.71238898038469), DroneState(10, 7, 0.0), DroneState(10, 7, 1.5707963267948966), DroneState(10, 7, 3.141592653589793), DroneState(10, 7, 4.71238898038469), DroneState(2, 6, 0.0), DroneState(2, 6, 1.5707963267948966), DroneState(2, 6, 3.141592653589793), DroneState(2, 6, 4.71238898038469), DroneState(3, 6, 0.0), DroneState(3, 6, 1.5707963267948966), DroneState(3, 6, 3.141592653589793), DroneState(3, 6, 4.71238898038469), DroneState(6, 6, 0.0), DroneState(6, 6, 1.5707963267948966), DroneState(6, 6, 3.141592653589793), DroneState(6, 6, 4.71238898038469), DroneState(7, 6, 0.0), DroneState(7, 6, 1.5707963267948966), DroneState(7, 6, 3.141592653589793), DroneState(7, 6, 4.71238898038469), DroneState(8, 6, 0.0), DroneState(8, 6, 1.5707963267948966), DroneState(8, 6, 3.141592653589793), DroneState(8, 6, 4.71238898038469), DroneState(9, 6, 0.0), DroneState(9, 6, 1.5707963267948966), DroneState(9, 6, 3.141592653589793), DroneState(9, 6, 4.71238898038469), DroneState(10, 6, 0.0), DroneState(10, 6, 1.5707963267948966), DroneState(10, 6, 3.141592653589793), DroneState(10, 6, 4.71238898038469), DroneState(10, 5, 0.0), DroneState(10, 5, 1.5707963267948966), DroneState(10, 5, 3.141592653589793), DroneState(10, 5, 4.71238898038469), DroneState(10, 4, 0.0), DroneState(10, 4, 1.5707963267948966), DroneState(10, 4, 3.141592653589793), DroneState(10, 4, 4.71238898038469), DroneState(2, 3, 0.0), DroneState(2, 3, 1.5707963267948966), DroneState(2, 3, 3.141592653589793), DroneState(2, 3, 4.71238898038469), DroneState(3, 3, 0.0), DroneState(3, 3, 1.5707963267948966), DroneState(3, 3, 3.141592653589793), DroneState(3, 3, 4.71238898038469), DroneState(2, 2, 0.0), DroneState(2, 2, 1.5707963267948966), DroneState(2, 2, 3.141592653589793), DroneState(2, 2, 4.71238898038469), DroneState(3, 2, 0.0), DroneState(3, 2, 1.5707963267948966), DroneState(3, 2, 3.141592653589793), DroneState(3, 2, 4.71238898038469), DroneState(4, 2, 0.0), DroneState(4, 2, 1.5707963267948966), DroneState(4, 2, 3.141592653589793), DroneState(4, 2, 4.71238898038469), DroneState(5, 2, 0.0), DroneState(5, 2, 1.5707963267948966), DroneState(5, 2, 3.141592653589793), DroneState(5, 2, 4.71238898038469), DroneState(6, 2, 0.0), DroneState(6, 2, 1.5707963267948966), DroneState(6, 2, 3.141592653589793), DroneState(6, 2, 4.71238898038469), DroneState(7, 2, 0.0), DroneState(7, 2, 1.5707963267948966), DroneState(7, 2, 3.141592653589793), DroneState(7, 2, 4.71238898038469)], # reward states\n",
    "                   rv::Vector{Float64}=[100.0, 100.0, 100.0, 100.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0], # reward values\n",
    "                   tp::Float64=1.0, # tprob\n",
    "                   discount_factor::Float64=0.9,\n",
    "                   target::Vector{Int64}=[10,10])\n",
    "    return DroneEnv(sx, sy,stheta, rs, rv, tp, discount_factor, target)\n",
    "end\n",
    "\n",
    "# we can now create a GridWorld mdp instance like this:\n",
    "mdp = DroneEnv()\n",
    "mdp.reward_states # mdp contains all the defualt values from the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action space\n",
    "function POMDPs.states(mdp::DroneEnv)\n",
    "    s = DroneState[DroneState(-1,-1,0.0)] # initialize an array of GridWorldStates\n",
    "    # loop over all our states, remeber there are two binary variables:\n",
    "    # done (d)\n",
    "    for d = 0:1,theta = 0.0:pi/2:3*pi/2,  y = 1:mdp.size_y, x = 1:mdp.size_x\n",
    "        push!(s, DroneState(x,y,theta,d))\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DroneState(10, 10, 0.0, false)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mdp = DroneEnv()\n",
    "state_space = states(mdp);\n",
    "state_space[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.actions(mdp::DroneEnv) = [:fwd, :bkwd, :l, :r, :ccw, :cw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition helpers\n",
    "function inbounds(mdp::DroneEnv,x::Int64,y::Int64)\n",
    "    if 1 <= x <= mdp.size_x && 1 <= y <= mdp.size_y\n",
    "        return true\n",
    "    else\n",
    "        return false\n",
    "    end\n",
    "end\n",
    "inbounds(mdp::DroneEnv, state::DroneState) = inbounds(mdp, state.x, state.y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.transition(mdp::DroneEnv, state::DroneState, action::Symbol)\n",
    "    a = action\n",
    "    x = state.x\n",
    "    y = state.y\n",
    "    θ = Float64(state.theta)\n",
    "\n",
    "    if state.done\n",
    "        return SparseCat([DroneState(x, y, θ, true)], [1.0])\n",
    "    elseif state in mdp.reward_states\n",
    "        return SparseCat([DroneState(x, y, θ, true)], [1.0])\n",
    "    end\n",
    "\n",
    "    neighbors = [\n",
    "        DroneState(x+1, y, θ, false),\n",
    "        DroneState(x-1, y, θ, false), \n",
    "        DroneState(x, y+1, θ, false),\n",
    "        DroneState(x, y-1, θ, false),\n",
    "        DroneState(x, y, mod(θ+pi/2,2*pi), false),\n",
    "        DroneState(x, y, mod(θ-pi/2,2*pi), false)\n",
    "        ]\n",
    "    for i in 1:length(neighbors)\n",
    "        if neighbors[i].x == mdp.target[1] && neighbors[i].y == mdp.target[2]\n",
    "            neighbors[i] = DroneState(neighbors[i].x, neighbors[i].y, neighbors[i].theta, true)\n",
    "        end\n",
    "    end\n",
    "    if θ == 0.0\n",
    "        targets = Dict(:fwd=>1, :bkwd=>2, :l=>3, :r=>4, :ccw=>5, :cw=>6)\n",
    "    elseif θ == pi/2\n",
    "        targets = Dict(:fwd=>3, :bkwd=>4, :l=>2, :r=>1, :ccw=>5, :cw=>6)\n",
    "    elseif θ == Float64(pi)\n",
    "        targets = Dict(:fwd=>2, :bkwd=>1, :l=>4, :r=>3, :ccw=>5, :cw=>6)\n",
    "    elseif θ == 3*pi/2\n",
    "        targets = Dict(:fwd=>4, :bkwd=>3, :l=>1, :r=>2, :ccw=>5, :cw=>6)\n",
    "    end\n",
    "    target = targets[a]\n",
    "    \n",
    "    probability = fill(0.0, 6)\n",
    "    if !inbounds(mdp, neighbors[target])\n",
    "        # If would transition out of bounds, stay in\n",
    "        # same cell with probability 1\n",
    "        return SparseCat([DroneState(-1, -1, 0.0, true)], [1.0])\n",
    "    else\n",
    "        probability[target] = mdp.tprob\n",
    "\n",
    "        oob_count = sum(!inbounds(mdp, n) for n in neighbors) # number of out of bounds neighbors\n",
    "\n",
    "        new_probability = (1.0 - mdp.tprob)/(3-oob_count)\n",
    "\n",
    "        for i = 1:6 # do not include neighbor 5\n",
    "            if inbounds(mdp, neighbors[i]) && i != target\n",
    "                probability[i] = new_probability\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return SparseCat(neighbors, probability)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.reward(mdp::DroneEnv, state::DroneState, action::Symbol, statep::DroneState) #deleted action\n",
    "    if state.done\n",
    "        return 0.0\n",
    "    end\n",
    "    if action == :fwd\n",
    "        r = -1\n",
    "    elseif action == :bkwd\n",
    "        r = -2\n",
    "    elseif action == :l || action == :r\n",
    "        r = -6\n",
    "    else\n",
    "        r = -4\n",
    "    end\n",
    "    \n",
    "    n = length(mdp.reward_states)\n",
    "    for i = 1:n\n",
    "        if posequal(statep, mdp.reward_states[i])\n",
    "            r = mdp.reward_values[i]\n",
    "        elseif !inbounds(mdp, statep)\n",
    "            r = -1000\n",
    "        end\n",
    "    end\n",
    "    return r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.discount(mdp::DroneEnv) = mdp.discount_factor;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.stateindex(mdp::DroneEnv, state::DroneState)\n",
    "    num_x = mdp.size_x\n",
    "    num_y = mdp.size_y\n",
    "    num_theta = mdp.size_theta\n",
    "    if state == DroneState(-1,-1,0.0)\n",
    "        return 1\n",
    "    end\n",
    "\n",
    "    # Define a mapping for theta values to integers\n",
    "    theta_mapping = Dict(0.0 => 1, Float64(pi)/2 => 2, Float64(pi) => 3, 3*Float64(pi)/2 => 4)\n",
    "\n",
    "    # Calculate the index based on state variables\n",
    "    theta_index = theta_mapping[state.theta]\n",
    "    index = state.x + (state.y - 1) * num_x + (theta_index - 1) * num_x * num_y\n",
    "\n",
    "    # If the state is terminal, add the size of the non-terminal states\n",
    "    if state.done\n",
    "        index += num_x * num_y * num_theta\n",
    "    end\n",
    "    return index\n",
    "end\n",
    "\n",
    "function POMDPs.actionindex(mdp::DroneEnv, act::Symbol)\n",
    "    if act==:fwd\n",
    "        return 1\n",
    "    elseif act==:bkwd\n",
    "        return 2\n",
    "    elseif act==:l\n",
    "        return 3\n",
    "    elseif act==:r\n",
    "        return 4\n",
    "    elseif act==:ccw\n",
    "        return 5\n",
    "    elseif act==:cw\n",
    "        return 6\n",
    "    end\n",
    "    error(\"Invalid GridWorld action: $act\")\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.isterminal(mdp::DroneEnv, s::DroneState) = s.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.initialstate(pomdp::DroneEnv) = Deterministic(DroneState(1, 1, 0.0)) # TODO: define initialistate for states, not distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "render (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# render\n",
    "function render(mdp::DroneEnv, state::DroneState)\n",
    "    p = plot(size=(600, 600), xlim=(0, mdp.size_x+1), ylim=(0, mdp.size_y+1), legend=false)\n",
    "    xticks!(0:0.5:mdp.size_x+1)\n",
    "    yticks!(0:0.5:mdp.size_y+1)\n",
    "    for i in 1:length(mdp.reward_states)\n",
    "        if mdp.reward_values[i] < 1\n",
    "            plot!([mdp.reward_states[i].x], [mdp.reward_states[i].y], mark=:circle, markersize=20, color=:black)\n",
    "        end\n",
    "    end\n",
    "    plot!([mdp.target[1]], [mdp.target[2]], mark=:star, markersize=20, color=:yellow)\n",
    "    plot!([state.x], [state.y], mark=:diamond, markersize=20, color=:blue)\n",
    "    quiver!([state.x], [state.y], quiver=[(0.5*cos(state.theta), 0.5*sin(state.theta))], color=:black, arrow=true, linewidth=3)\n",
    "    \n",
    "    plot!([0.5, 0.5], [0.5, 10.5], color=:black, linewidth=2)  # (0.5,0.5) to (0.5,10.5)\n",
    "    plot!([0.5, 10.5], [0.5, 0.5], color=:black, linewidth=2)  # (0.5,0.5) to (10.5,0.5)\n",
    "    plot!([0.5, 10.5], [10.5, 10.5], color=:black, linewidth=2)  # (0.5,10.5) to (10.5,10.5)\n",
    "    plot!([10.5, 10.5], [10.5, 0.5], color=:black, linewidth=2)  # (10.5,10.5) to (10.5,0.5)\n",
    "\n",
    "    display(p)\n",
    "    println(state)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration is a dynamic porgramming apporach for solving MDPs. See the [wikipedia](https://en.wikipedia.org/wiki/Markov_decision_process#Value_iteration) article for a brief explanation. The solver can be found [here](https://github.com/JuliaPOMDP/DiscreteValueIteration.jl). If you haven't isntalled the solver yet, you can run the following from the Julia REPL to download the module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "POMDPs.add(\"DiscreteValueIteration\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each POMDPs.jl solver provides two data types for you to interface with. The first is the Solver type which contains solver parameters. The second is the Policy type. Let's see hwo we can use them to get an optimal action at a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Problem creating an ordered vector of states in ordered_states(...). There is likely a mistake in stateindex(...) or n_states(...).\n",
      "│ \n",
      "│ n_states(...) was 801.\n",
      "│ \n",
      "│ states corresponding to the following indices were missing from states(...): [801]\n",
      "└ @ POMDPTools.ModelTools /Users/andres/.julia/packages/POMDPTools/7Rekv/src/ModelTools/ordered_spaces.jl:50\n"
     ]
    }
   ],
   "source": [
    "# first let's load the value iteration module\n",
    "using DiscreteValueIteration\n",
    "\n",
    "# initialize the problem\n",
    "mdp = DroneEnv()\n",
    "\n",
    "# initialize the solver\n",
    "# max_iterations: maximum number of iterations value iteration runs for (default is 100)\n",
    "# belres: the value of Bellman residual used in the solver (defualt is 1e-6)\n",
    "solver = ValueIterationSolver(max_iterations=1_000_000, belres=1e-3; verbose=false)\n",
    "\n",
    "# solve for an optimal policy\n",
    "policy = solve(solver, mdp); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the path an agent may take in the gridworld, starting in the initial state (set to `(1,1)`), you may run following code. Note that the policy differs from the example above, as the gridworld in the image has -1 reward for accidentaly trying to move into the wall, which we have not implemented in our MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s = DroneState(1, 1, 0.0, false)\n",
      "a = :ccw\n",
      "r = -4\n",
      "rt = -4.0\n",
      "\n",
      "s = DroneState(1, 1, 1.5707963267948966, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -5.0\n",
      "\n",
      "s = DroneState(1, 2, 1.5707963267948966, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -6.0\n",
      "\n",
      "s = DroneState(1, 3, 1.5707963267948966, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -7.0\n",
      "\n",
      "s = DroneState(1, 4, 1.5707963267948966, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -8.0\n",
      "\n",
      "s = DroneState(1, 5, 1.5707963267948966, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -9.0\n",
      "\n",
      "s = DroneState(1, 6, 1.5707963267948966, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -10.0\n",
      "\n",
      "s = DroneState(1, 7, 1.5707963267948966, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -11.0\n",
      "\n",
      "s = DroneState(1, 8, 1.5707963267948966, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -12.0\n",
      "\n",
      "s = DroneState(1, 9, 1.5707963267948966, false)\n",
      "a = :cw\n",
      "r = -4\n",
      "rt = -16.0\n",
      "\n",
      "s = DroneState(1, 9, 0.0, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -17.0\n",
      "\n",
      "s = DroneState(2, 9, 0.0, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -18.0\n",
      "\n",
      "s = DroneState(3, 9, 0.0, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -19.0\n",
      "\n",
      "s = DroneState(4, 9, 0.0, false)\n",
      "a = :r\n",
      "r = -6\n",
      "rt = -25.0\n",
      "\n",
      "s = DroneState(4, 8, 0.0, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -26.0\n",
      "\n",
      "s = DroneState(5, 8, 0.0, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -27.0\n",
      "\n",
      "s = DroneState(6, 8, 0.0, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -28.0\n",
      "\n",
      "s = DroneState(7, 8, 0.0, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -29.0\n",
      "\n",
      "s = DroneState(8, 8, 0.0, false)\n",
      "a = :fwd\n",
      "r = -1\n",
      "rt = -30.0\n",
      "\n",
      "s = DroneState(9, 8, 0.0, false)\n",
      "a = :l\n",
      "r = -6\n",
      "rt = -36.0\n",
      "\n",
      "s = DroneState(9, 9, 0.0, false)\n",
      "a = :l\n",
      "r = -6\n",
      "rt = -42.0\n",
      "\n",
      "s = DroneState(9, 10, 0.0, false)\n",
      "a = :fwd\n",
      "r = 100.0\n",
      "rt = 58.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rt = 0.0\n",
    "for (s,a,r) in stepthrough(mdp, policy, \"s,a,r\", max_steps=100)\n",
    "    @show s\n",
    "    @show a\n",
    "    @show r\n",
    "    rt += r\n",
    "    @show rt \n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate using the planner to determine a good action at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total discounted reward: -4.680414708487483\n"
     ]
    }
   ],
   "source": [
    "s = DroneState(1,1, 0.0) # this is our starting state\n",
    "hist = HistoryRecorder(max_steps=1000)\n",
    "\n",
    "hist = simulate(hist, mdp, policy, s)\n",
    "\n",
    "println(\"Total discounted reward: $(discounted_reward(hist))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can view the state-action history using the `eachstep` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: DroneState(1, 1, 0.0, false)  a: ccw     s': DroneState(1, 1, 1.5707963267948966, false) r': -4.0                       rt': -4.0                      \n",
      "DroneState(1, 1, 1.5707963267948966, false)\n",
      "s: DroneState(1, 1, 1.5707963267948966, false)  a: fwd     s': DroneState(1, 2, 1.5707963267948966, false) r': -1.0                       rt': -5.0                      \n",
      "DroneState(1, 2, 1.5707963267948966, false)\n",
      "s: DroneState(1, 2, 1.5707963267948966, false)  a: fwd     s': DroneState(1, 3, 1.5707963267948966, false) r': -1.0                       rt': -6.0                      \n",
      "DroneState(1, 3, 1.5707963267948966, false)\n",
      "s: DroneState(1, 3, 1.5707963267948966, false)  a: fwd     s': DroneState(1, 4, 1.5707963267948966, false) r': -1.0                       rt': -7.0                      \n",
      "DroneState(1, 4, 1.5707963267948966, false)\n",
      "s: DroneState(1, 4, 1.5707963267948966, false)  a: fwd     s': DroneState(1, 5, 1.5707963267948966, false) r': -1.0                       rt': -8.0                      \n",
      "DroneState(1, 5, 1.5707963267948966, false)\n",
      "s: DroneState(1, 5, 1.5707963267948966, false)  a: fwd     s': DroneState(1, 6, 1.5707963267948966, false) r': -1.0                       rt': -9.0                      \n",
      "DroneState(1, 6, 1.5707963267948966, false)\n",
      "s: DroneState(1, 6, 1.5707963267948966, false)  a: fwd     s': DroneState(1, 7, 1.5707963267948966, false) r': -1.0                       rt': -10.0                     \n",
      "DroneState(1, 7, 1.5707963267948966, false)\n",
      "s: DroneState(1, 7, 1.5707963267948966, false)  a: fwd     s': DroneState(1, 8, 1.5707963267948966, false) r': -1.0                       rt': -11.0                     \n",
      "DroneState(1, 8, 1.5707963267948966, false)\n",
      "s: DroneState(1, 8, 1.5707963267948966, false)  a: fwd     s': DroneState(1, 9, 1.5707963267948966, false) r': -1.0                       rt': -12.0                     \n",
      "DroneState(1, 9, 1.5707963267948966, false)\n",
      "s: DroneState(1, 9, 1.5707963267948966, false)  a: cw      s': DroneState(1, 9, 0.0, false) r': -4.0                       rt': -16.0                     \n",
      "DroneState(1, 9, 0.0, false)\n",
      "s: DroneState(1, 9, 0.0, false)  a: fwd     s': DroneState(2, 9, 0.0, false) r': -1.0                       rt': -17.0                     \n",
      "DroneState(2, 9, 0.0, false)\n",
      "s: DroneState(2, 9, 0.0, false)  a: fwd     s': DroneState(3, 9, 0.0, false) r': -1.0                       rt': -18.0                     \n",
      "DroneState(3, 9, 0.0, false)\n",
      "s: DroneState(3, 9, 0.0, false)  a: fwd     s': DroneState(4, 9, 0.0, false) r': -1.0                       rt': -19.0                     \n",
      "DroneState(4, 9, 0.0, false)\n",
      "s: DroneState(4, 9, 0.0, false)  a: r       s': DroneState(4, 8, 0.0, false) r': -6.0                       rt': -25.0                     \n",
      "DroneState(4, 8, 0.0, false)\n",
      "s: DroneState(4, 8, 0.0, false)  a: fwd     s': DroneState(5, 8, 0.0, false) r': -1.0                       rt': -26.0                     \n",
      "DroneState(5, 8, 0.0, false)\n",
      "s: DroneState(5, 8, 0.0, false)  a: fwd     s': DroneState(6, 8, 0.0, false) r': -1.0                       rt': -27.0                     \n",
      "DroneState(6, 8, 0.0, false)\n",
      "s: DroneState(6, 8, 0.0, false)  a: fwd     s': DroneState(7, 8, 0.0, false) r': -1.0                       rt': -28.0                     \n",
      "DroneState(7, 8, 0.0, false)\n",
      "s: DroneState(7, 8, 0.0, false)  a: fwd     s': DroneState(8, 8, 0.0, false) r': -1.0                       rt': -29.0                     \n",
      "DroneState(8, 8, 0.0, false)\n",
      "s: DroneState(8, 8, 0.0, false)  a: fwd     s': DroneState(9, 8, 0.0, false) r': -1.0                       rt': -30.0                     \n",
      "DroneState(9, 8, 0.0, false)\n",
      "s: DroneState(9, 8, 0.0, false)  a: l       s': DroneState(9, 9, 0.0, false) r': -6.0                       rt': -36.0                     \n",
      "DroneState(9, 9, 0.0, false)\n",
      "s: DroneState(9, 9, 0.0, false)  a: l       s': DroneState(9, 10, 0.0, false) r': -6.0                       rt': -42.0                     \n",
      "DroneState(9, 10, 0.0, false)\n",
      "s: DroneState(9, 10, 0.0, false)  a: fwd     s': DroneState(10, 10, 0.0, true) r': 100.0                      rt': 58.0                      \n",
      "DroneState(10, 10, 0.0, true)\n"
     ]
    }
   ],
   "source": [
    "using Printf\n",
    "rt = 0 \n",
    "for (s, a, sp, r) in eachstep(hist, \"s,a,sp,r\")\n",
    "    rt += r\n",
    "    @printf(\"s: %-26s  a: %-6s  s': %-26s r': %-26s rt': %-26s\\n\", s, a, sp, r, rt)\n",
    "    render(mdp,sp)\n",
    "    sleep(0.25)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
