{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# first import the POMDPs.jl interface\n",
    "using POMDPs\n",
    "using Plots\n",
    "# POMDPModelTools has tools that help build the MDP definition\n",
    "using POMDPModelTools\n",
    "# POMDPPolicies provides functions to help define simple policies\n",
    "using POMDPPolicies\n",
    "# POMDPSimulators provide functions for running MDP simulations\n",
    "using POMDPSimulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct DroneState\n",
    "    x::Int64 # x position\n",
    "    y::Int64 # y position\n",
    "    theta::Float64 # heading angle\n",
    "    done::Bool # are we in a terminal state?\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# initial state constructor\n",
    "DroneState(x::Int64, y::Int64, θ::Float64) = DroneState(x,y,θ,false)\n",
    "# checks if the position of two states are the same\n",
    "posequal(s1::DroneState, s2::DroneState) = s1.x == s2.x && s1.y == s2.y && s1.theta == s2.theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the grid world mdp type\n",
    "mutable struct DroneEnv <: MDP{DroneState, Symbol}\n",
    "    size_x::Int64 # x size of the grid\n",
    "    size_y::Int64 # y size of the grid\n",
    "    size_theta::Int64 # theta size\n",
    "    reward_states::Vector{DroneState} # the states in which agent recieves reward\n",
    "    reward_values::Vector{Float64} # reward values for those states\n",
    "    tprob::Float64 # probability of transitioning to the desired state\n",
    "    discount_factor::Float64 # disocunt factor\n",
    "    target::Vector{Int64}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#we use key worded arguments so we can change any of the values we pass in \n",
    "function DroneEnv(;sx::Int64=10, # size_x\n",
    "                   sy::Int64=10, # size_y\n",
    "                   stheta::Int64=4, # size_theta\n",
    "                   rs::Vector{DroneState}=[DroneState(10, 10, 0.0), DroneState(10, 10, 1.5707963267948966), DroneState(10, 10, 3.141592653589793), DroneState(10, 10, 4.71238898038469), DroneState(5, 10, 0.0), DroneState(5, 10, 1.5707963267948966), DroneState(5, 10, 3.141592653589793), DroneState(5, 10, 4.71238898038469), DroneState(5, 9, 0.0), DroneState(5, 9, 1.5707963267948966), DroneState(5, 9, 3.141592653589793), DroneState(5, 9, 4.71238898038469), DroneState(10, 9, 0.0), DroneState(10, 9, 1.5707963267948966), DroneState(10, 9, 3.141592653589793), DroneState(10, 9, 4.71238898038469), DroneState(2, 8, 0.0), DroneState(2, 8, 1.5707963267948966), DroneState(2, 8, 3.141592653589793), DroneState(2, 8, 4.71238898038469), DroneState(3, 8, 0.0), DroneState(3, 8, 1.5707963267948966), DroneState(3, 8, 3.141592653589793), DroneState(3, 8, 4.71238898038469), DroneState(10, 8, 0.0), DroneState(10, 8, 1.5707963267948966), DroneState(10, 8, 3.141592653589793), DroneState(10, 8, 4.71238898038469), DroneState(2, 7, 0.0), DroneState(2, 7, 1.5707963267948966), DroneState(2, 7, 3.141592653589793), DroneState(2, 7, 4.71238898038469), DroneState(3, 7, 0.0), DroneState(3, 7, 1.5707963267948966), DroneState(3, 7, 3.141592653589793), DroneState(3, 7, 4.71238898038469), DroneState(10, 7, 0.0), DroneState(10, 7, 1.5707963267948966), DroneState(10, 7, 3.141592653589793), DroneState(10, 7, 4.71238898038469), DroneState(2, 6, 0.0), DroneState(2, 6, 1.5707963267948966), DroneState(2, 6, 3.141592653589793), DroneState(2, 6, 4.71238898038469), DroneState(3, 6, 0.0), DroneState(3, 6, 1.5707963267948966), DroneState(3, 6, 3.141592653589793), DroneState(3, 6, 4.71238898038469), DroneState(6, 6, 0.0), DroneState(6, 6, 1.5707963267948966), DroneState(6, 6, 3.141592653589793), DroneState(6, 6, 4.71238898038469), DroneState(7, 6, 0.0), DroneState(7, 6, 1.5707963267948966), DroneState(7, 6, 3.141592653589793), DroneState(7, 6, 4.71238898038469), DroneState(8, 6, 0.0), DroneState(8, 6, 1.5707963267948966), DroneState(8, 6, 3.141592653589793), DroneState(8, 6, 4.71238898038469), DroneState(9, 6, 0.0), DroneState(9, 6, 1.5707963267948966), DroneState(9, 6, 3.141592653589793), DroneState(9, 6, 4.71238898038469), DroneState(10, 6, 0.0), DroneState(10, 6, 1.5707963267948966), DroneState(10, 6, 3.141592653589793), DroneState(10, 6, 4.71238898038469), DroneState(10, 5, 0.0), DroneState(10, 5, 1.5707963267948966), DroneState(10, 5, 3.141592653589793), DroneState(10, 5, 4.71238898038469), DroneState(10, 4, 0.0), DroneState(10, 4, 1.5707963267948966), DroneState(10, 4, 3.141592653589793), DroneState(10, 4, 4.71238898038469), DroneState(2, 3, 0.0), DroneState(2, 3, 1.5707963267948966), DroneState(2, 3, 3.141592653589793), DroneState(2, 3, 4.71238898038469), DroneState(3, 3, 0.0), DroneState(3, 3, 1.5707963267948966), DroneState(3, 3, 3.141592653589793), DroneState(3, 3, 4.71238898038469), DroneState(2, 2, 0.0), DroneState(2, 2, 1.5707963267948966), DroneState(2, 2, 3.141592653589793), DroneState(2, 2, 4.71238898038469), DroneState(3, 2, 0.0), DroneState(3, 2, 1.5707963267948966), DroneState(3, 2, 3.141592653589793), DroneState(3, 2, 4.71238898038469), DroneState(4, 2, 0.0), DroneState(4, 2, 1.5707963267948966), DroneState(4, 2, 3.141592653589793), DroneState(4, 2, 4.71238898038469), DroneState(5, 2, 0.0), DroneState(5, 2, 1.5707963267948966), DroneState(5, 2, 3.141592653589793), DroneState(5, 2, 4.71238898038469), DroneState(6, 2, 0.0), DroneState(6, 2, 1.5707963267948966), DroneState(6, 2, 3.141592653589793), DroneState(6, 2, 4.71238898038469), DroneState(7, 2, 0.0), DroneState(7, 2, 1.5707963267948966), DroneState(7, 2, 3.141592653589793), DroneState(7, 2, 4.71238898038469)], # reward states\n",
    "                   rv::Vector{Float64}=[100.0, 100.0, 100.0, 100.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0, -1000.0], # reward values\n",
    "                   tp::Float64=1.0, # tprob\n",
    "                   discount_factor::Float64=0.9,\n",
    "                   target::Vector{Int64}=[10,10])\n",
    "    return DroneEnv(sx, sy,stheta, rs, rv, tp, discount_factor, target)\n",
    "end\n",
    "\n",
    "# we can now create a GridWorld mdp instance like this:\n",
    "mdp = DroneEnv()\n",
    "mdp.reward_states # mdp contains all the defualt values from the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action space\n",
    "function POMDPs.states(mdp::DroneEnv)\n",
    "    s = DroneState[DroneState(-1,-1,0.0)] # initialize an array of GridWorldStates\n",
    "    # loop over all our states, remeber there are two binary variables:\n",
    "    # done (d)\n",
    "    for d = 0:1,theta = 0.0:pi/2:3*pi/2,  y = 1:mdp.size_y, x = 1:mdp.size_x\n",
    "        push!(s, DroneState(x,y,theta,d))\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mdp = DroneEnv()\n",
    "state_space = states(mdp);\n",
    "state_space[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.actions(mdp::DroneEnv) = [:fwd, :bkwd, :l, :r, :ccw, :cw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition helpers\n",
    "function inbounds(mdp::DroneEnv,x::Int64,y::Int64)\n",
    "    if 1 <= x <= mdp.size_x && 1 <= y <= mdp.size_y\n",
    "        return true\n",
    "    else\n",
    "        return false\n",
    "    end\n",
    "end\n",
    "inbounds(mdp::DroneEnv, state::DroneState) = inbounds(mdp, state.x, state.y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.transition(mdp::DroneEnv, state::DroneState, action::Symbol)\n",
    "    @show state\n",
    "    a = action\n",
    "    x = state.x\n",
    "    y = state.y\n",
    "    θ = Float64(state.theta)\n",
    "\n",
    "    if state.done\n",
    "        return SparseCat([DroneState(x, y, θ, true)], [1.0])\n",
    "    elseif state in mdp.reward_states\n",
    "        return SparseCat([DroneState(x, y, θ, true)], [1.0])\n",
    "    end\n",
    "\n",
    "    neighbors = [\n",
    "        DroneState(x+1, y, θ, false),\n",
    "        DroneState(x-1, y, θ, false), \n",
    "        DroneState(x, y+1, θ, false),\n",
    "        DroneState(x, y-1, θ, false),\n",
    "        DroneState(x, y, mod(θ+pi/2,2*pi), false),\n",
    "        DroneState(x, y, mod(θ-pi/2,2*pi), false)\n",
    "        ]\n",
    "    for i in 1:length(neighbors)\n",
    "        if neighbors[i].x == mdp.target[1] && neighbors[i].y == mdp.target[2]\n",
    "            neighbors[i] = DroneState(neighbors[i].x, neighbors[i].y, neighbors[i].theta, true)\n",
    "        end\n",
    "    end\n",
    "    if θ == 0.0\n",
    "        targets = Dict(:fwd=>1, :bkwd=>2, :l=>3, :r=>4, :ccw=>5, :cw=>6)\n",
    "    elseif θ == pi/2\n",
    "        targets = Dict(:fwd=>3, :bkwd=>4, :l=>2, :r=>1, :ccw=>5, :cw=>6)\n",
    "    elseif θ == Float64(pi)\n",
    "        targets = Dict(:fwd=>2, :bkwd=>1, :l=>4, :r=>3, :ccw=>5, :cw=>6)\n",
    "    elseif θ == 3*pi/2\n",
    "        targets = Dict(:fwd=>4, :bkwd=>3, :l=>1, :r=>2, :ccw=>5, :cw=>6)\n",
    "    end\n",
    "    target = targets[a]\n",
    "    \n",
    "    probability = fill(0.0, 6)\n",
    "    if !inbounds(mdp, neighbors[target])\n",
    "        # If would transition out of bounds, stay in\n",
    "        # same cell with probability 1\n",
    "        return SparseCat([DroneState(-1, -1, 0.0, true)], [1.0])\n",
    "    else\n",
    "        probability[target] = mdp.tprob\n",
    "\n",
    "        oob_count = sum(!inbounds(mdp, n) for n in neighbors) # number of out of bounds neighbors\n",
    "\n",
    "        new_probability = (1.0 - mdp.tprob)/(3-oob_count)\n",
    "\n",
    "        for i = 1:6 # do not include neighbor 5\n",
    "            if inbounds(mdp, neighbors[i]) && i != target\n",
    "                probability[i] = new_probability\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return SparseCat(neighbors, probability)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.reward(mdp::DroneEnv, state::DroneState, action::Symbol, statep::DroneState) #deleted action\n",
    "    if state.done\n",
    "        return 0.0\n",
    "    end\n",
    "    if state.x==statep.x && state.y==statep.y\n",
    "        return -10\n",
    "    end\n",
    "\n",
    "    r = -1\n",
    "    n = length(mdp.reward_states)\n",
    "    for i = 1:n\n",
    "        if posequal(statep, mdp.reward_states[i])\n",
    "            r = mdp.reward_values[i]\n",
    "        elseif !inbounds(mdp, statep)\n",
    "            r = -1000\n",
    "        end\n",
    "    end\n",
    "    return r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.discount(mdp::DroneEnv) = mdp.discount_factor;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.stateindex(mdp::DroneEnv, state::DroneState)\n",
    "    num_x = mdp.size_x\n",
    "    num_y = mdp.size_y\n",
    "    num_theta = mdp.size_theta\n",
    "    if state == DroneState(-1,-1,0.0)\n",
    "        return 1\n",
    "    end\n",
    "\n",
    "    # Define a mapping for theta values to integers\n",
    "    theta_mapping = Dict(0.0 => 1, Float64(pi)/2 => 2, Float64(pi) => 3, 3*Float64(pi)/2 => 4)\n",
    "\n",
    "    # Calculate the index based on state variables\n",
    "    theta_index = theta_mapping[state.theta]\n",
    "    index = state.x + (state.y - 1) * num_x + (theta_index - 1) * num_x * num_y\n",
    "\n",
    "    # If the state is terminal, add the size of the non-terminal states\n",
    "    if state.done\n",
    "        index += num_x * num_y * num_theta\n",
    "    end\n",
    "    return index\n",
    "end\n",
    "\n",
    "function POMDPs.actionindex(mdp::DroneEnv, act::Symbol)\n",
    "    if act==:fwd\n",
    "        return 1\n",
    "    elseif act==:bkwd\n",
    "        return 2\n",
    "    elseif act==:l\n",
    "        return 3\n",
    "    elseif act==:r\n",
    "        return 4\n",
    "    elseif act==:ccw\n",
    "        return 5\n",
    "    elseif act==:cw\n",
    "        return 6\n",
    "    end\n",
    "    error(\"Invalid GridWorld action: $act\")\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.isterminal(mdp::DroneEnv, s::DroneState) = s.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.initialstate(pomdp::DroneEnv) = Deterministic(DroneState(1, 1, 0.0)) # TODO: define initialistate for states, not distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render\n",
    "function render(mdp::DroneEnv, state::DroneState)\n",
    "    p = plot(size=(800, 800), xlim=(0, mdp.size_x+1), ylim=(0, mdp.size_y+1), legend=false)\n",
    "    xticks!(0:0.5:mdp.size_x+1)\n",
    "    yticks!(0:0.5:mdp.size_y+1)\n",
    "    for i in 1:length(mdp.reward_states)\n",
    "        if mdp.reward_values[i] < 1\n",
    "            plot!([mdp.reward_states[i].x], [mdp.reward_states[i].y], mark=:circle, markersize=20, color=:black)\n",
    "        end\n",
    "    end\n",
    "    plot!(mdp.target[1], mdp.target[2], mark=:star, markersize=20, color=:yellow)\n",
    "    plot!([state.x], [state.y], mark=:diamond, markersize=20, color=:blue)\n",
    "    quiver!([state.x], [state.y], quiver=[(0.5*cos(state.theta), 0.5*sin(state.theta))], color=:black, arrow=true, linewidth=5)\n",
    "    \n",
    "    plot!([0.5, 0.5], [0.5, 10.5], color=:black, linewidth=2)  # (0.5,0.5) to (0.5,10.5)\n",
    "    plot!([0.5, 10.5], [0.5, 0.5], color=:black, linewidth=2)  # (0.5,0.5) to (10.5,0.5)\n",
    "    plot!([0.5, 10.5], [10.5, 10.5], color=:black, linewidth=2)  # (0.5,10.5) to (10.5,10.5)\n",
    "    plot!([10.5, 10.5], [10.5, 0.5], color=:black, linewidth=2)  # (10.5,10.5) to (10.5,0.5)\n",
    "\n",
    "    display(p)\n",
    "    println(state)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = DroneEnv()\n",
    "s = DroneState(9, 10, 4.71238898038469, false)\n",
    "a = :l\n",
    "sp = transition(mdp,s,a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp =rand(sp)\n",
    "@show reward(mdp,s,a,sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(mdp,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulations\n",
    "\n",
    "Now that we have defined the problem, we should simulate it to see it working. The funcion `stepthrough` from `POMDPSimulators` provides a convenient `for` loop syntax for exploring the behavior of the mdp. The `POMDPPolicies` provides a way for defining simple policies, such as `RandomPolicy` or `FunctionPolicy` that takes a lambda function to determine the next action based on the state `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mdp = DroneEnv()\n",
    "mdp.tprob=1.0\n",
    "\n",
    "policy = RandomPolicy(mdp)\n",
    "left_policy = FunctionPolicy(s->:l)\n",
    "right_policy = FunctionPolicy(s->:r)\n",
    "\n",
    "for (s,a,r) in stepthrough(mdp, right_policy, \"s,a,r\", max_steps=10)\n",
    "    @show s\n",
    "    @show a\n",
    "    @show r\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration is a dynamic porgramming apporach for solving MDPs. See the [wikipedia](https://en.wikipedia.org/wiki/Markov_decision_process#Value_iteration) article for a brief explanation. The solver can be found [here](https://github.com/JuliaPOMDP/DiscreteValueIteration.jl). If you haven't isntalled the solver yet, you can run the following from the Julia REPL to download the module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "POMDPs.add(\"DiscreteValueIteration\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each POMDPs.jl solver provides two data types for you to interface with. The first is the Solver type which contains solver parameters. The second is the Policy type. Let's see hwo we can use them to get an optimal action at a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# first let's load the value iteration module\n",
    "using DiscreteValueIteration\n",
    "\n",
    "# initialize the problem\n",
    "mdp = DroneEnv()\n",
    "\n",
    "# initialize the solver\n",
    "# max_iterations: maximum number of iterations value iteration runs for (default is 100)\n",
    "# belres: the value of Bellman residual used in the solver (defualt is 1e-3)\n",
    "solver = ValueIterationSolver(max_iterations=100_000, belres=1e-3; verbose=false)\n",
    "\n",
    "# solve for an optimal policy\n",
    "policy = solve(solver, mdp); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the policy along with the ```action(...)``` function to get the optimal action in a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# say we are in state (9,2)\n",
    "s = DroneState(9,7, 0.0)\n",
    "a = action(policy, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remeber that the state (9,3) has an immediate reward of +10.0, so the policy we found is moving up as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s = DroneState(9,10, Float64(pi))\n",
    "a = action(policy, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![description](gw.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the path an agent may take in the gridworld, starting in the initial state (set to `(1,1)`), you may run following code. Note that the policy differs from the example above, as the gridworld in the image has -1 reward for accidentaly trying to move into the wall, which we have not implemented in our MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (s,a,r) in stepthrough(mdp, policy, \"s,a,r\", max_steps=20)\n",
    "    @show s\n",
    "    @show a\n",
    "    @show r\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Tree Search Solver\n",
    "Monte-Carlo Tree Search (MCTS) is another MDP solver. It is an online method that looks for the best action from only the current state by building a search tree. A nice overview of MCTS can be found [here](http://www.diego-perez.net/papers/MCTSSurvey.pdf). Run the following command to donwload the module\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "POMDPs.add(\"MCTS\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly run through an example of using the solver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using MCTS\n",
    "\n",
    "# initialize the problem\n",
    "mdp = DroneEnv()\n",
    "\n",
    "# initialize the solver with hyper parameters\n",
    "# n_iterations: the number of iterations that each search runs for\n",
    "# depth: the depth of the tree (how far away from the current state the algorithm explores)\n",
    "# exploration constant: this is how much weight to put into exploratory actions. \n",
    "# A good rule of thumb is to set the exploration constant to what you expect the upper bound on your average expected reward to be.\n",
    "solver = MCTSSolver(n_iterations=100_000,\n",
    "                    depth=100,\n",
    "                    exploration_constant=10.0,\n",
    "                    enable_tree_vis=true)\n",
    "\n",
    "# initialize the planner by calling the `solve` function. For online solvers, the \n",
    "planner = solve(solver, mdp)\n",
    "\n",
    "# to get the action:\n",
    "s = DroneState(9,2, 0.0)\n",
    "a = action(planner, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate using the planner to determine a good action at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s = DroneState(1,1, 0.0) # this is our starting state\n",
    "hist = HistoryRecorder(max_steps=1000)\n",
    "\n",
    "hist = simulate(hist, mdp, policy, s)\n",
    "\n",
    "println(\"Total discounted reward: $(discounted_reward(hist))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can view the state-action history using the `eachstep` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "using Printf\n",
    "for (s, a, sp) in eachstep(hist, \"s,a,sp\")\n",
    "    @printf(\"s: %-26s  a: %-6s  s': %-26s\\n\", s, a, sp)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what the planner is doing, we can look at the tree created when it plans at a particular state, for example, the first state in the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "using D3Trees\n",
    "\n",
    "# first, run the planner on the state\n",
    "s = state_hist(hist)[1]\n",
    "a, info = action_info(planner, s);\n",
    "\n",
    "# show the tree (click the node to expand)\n",
    "D3Tree(info[:tree], init_expand=1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
