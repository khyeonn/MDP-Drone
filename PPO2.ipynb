{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Drone2.jl\")\n",
    "using .Drone2: DroneMDP, DroneAct, DroneState, render, gen, isterminal\n",
    "using StaticArrays\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "using Distributions: Normal, logpdf\n",
    "using Flux\n",
    "using Flux: params, gradient, update!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCritic"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "struct ActorCritic\n",
    "    actor::Chain\n",
    "    critic::Chain\n",
    "end\n",
    "\n",
    "function ActorCritic(state_size::Int, action_size::Int)\n",
    "    actor = Chain(\n",
    "        Dense(state_size, 64, sigmoid),\n",
    "        Dense(64, action_size * 2),\n",
    "        x -> [x[1],x[2], softplus.(x[3]),softplus.(x[4])]\n",
    "    )\n",
    "\n",
    "    critic = Chain(\n",
    "        Dense(state_size, 64, sigmoid),\n",
    "        Dense(64, 1)\n",
    "    )\n",
    "\n",
    "    return ActorCritic(actor, critic)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function forward(ac::ActorCritic, state::DroneState)\n",
    "    actor_out = ac.actor(state)\n",
    "    action_mean = [actor_out[1], actor_out[2]]\n",
    "    action_std = [actor_out[3], actor_out[4]]  \n",
    "    value = ac.critic(state)[1]\n",
    "    return [action_mean, action_std], value  # Concatenate means and std deviations into a single array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_prob (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function log_prob(ac::ActorCritic, states::Vector{DroneState}, actions::Vector{DroneAct})\n",
    "    log_probs = zeros(length(states))\n",
    "\n",
    "    for i in 1:length(states)\n",
    "        (action_mean, action_std), _ = forward(ac, states[i])\n",
    "        log_probs[i] = sum(logpdf.(Normal.(action_mean, action_std), actions[i]))\n",
    "    end\n",
    "\n",
    "    return log_probs\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simulation(m::DroneMDP, s0::DroneState, max_steps=Inf)\n",
    "    t = 0\n",
    "    s = s0\n",
    "    states::Vector{DroneState} = []\n",
    "    actions::Vector{DroneAct} = []\n",
    "    rewards = []\n",
    "    values = []\n",
    "    while !isterminal(m, s) && t < max_steps\n",
    "        (action_mean, action_std), value = forward(ac, s0)\n",
    "        push!(values,value)\n",
    "        a = DroneAct(rand.(Normal.(action_mean, action_std)))  # Call the policy_function\n",
    "        push!(actions,a)\n",
    "        s, r = gen(m,s,a)\n",
    "        push!(states,s)\n",
    "        push!(rewards,r)\n",
    "        \n",
    "        t += 1\n",
    "    end\n",
    "    \n",
    "    return states,actions,rewards,values  \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_advantage (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "function compute_advantage(r::Vector, values::Vector, discount::Float32, lambda::Float32)\n",
    "    T = length(r)\n",
    "    δ = r .+ discount * [values[2:end]; 0] .- values\n",
    "    advantages = similar(δ)\n",
    "    running_advantage = 0\n",
    "    for t in T:-1:1\n",
    "        running_advantage = δ[t] + discount * lambda * running_advantage\n",
    "        advantages[t] = running_advantage\n",
    "    end\n",
    "    return advantages\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ppo_loss (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function ppo_loss(ac::ActorCritic, states::Vector{DroneState}, actions::Vector{DroneAct}, rewards, values, discount, lambda, eps_clip)\n",
    "\n",
    "    # Compute advantages\n",
    "    advantages = compute_advantage(rewards, values, discount, lambda)\n",
    "\n",
    "    # Compute old and new log probabilities\n",
    "    old_log_probs = log_prob(ac, states, actions)\n",
    "    new_log_probs = log_prob(ac, states, actions)\n",
    "\n",
    "    # Compute ratio and clipped ratio\n",
    "    ratio = exp.(new_log_probs - old_log_probs)\n",
    "    clipped_ratio = clamp.(ratio, 1 - eps_clip, 1 + eps_clip)\n",
    "\n",
    "    # Compute surrogate loss\n",
    "    surrogate_loss = min.(advantages .* ratio, advantages .* clipped_ratio)\n",
    "\n",
    "    # Compute value loss\n",
    "    value_loss = sum((rewards .+ discount * [values[2:end]; 0] .- values).^2)\n",
    "\n",
    "    # Compute total loss\n",
    "    loss = -mean(surrogate_loss) + 0.5 * value_loss\n",
    "\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ppo_update! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function ppo_update!(ac::ActorCritic, optimizer, states, actions, rewards, values, discount, lambda, eps_clip)\n",
    "    gradient(params(ac)) do\n",
    "        loss = ppo_loss(ac, states, actions, rewards, values, discount, lambda, eps_clip)\n",
    "        return loss\n",
    "    end\n",
    "    Flux.Optimise.update!(optimizer, params(ac))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DroneMDP()\n",
    "ac = ActorCritic(4,2)\n",
    "s0 = DroneState(5,5,0.0,false)\n",
    "states1, actions1, rewards1, values1  =  simulation(m,s0,1000)\n",
    "lambda = Float32(0.99)\n",
    "eps_clip = Float32(0.2);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
